{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from chromadb import PersistentClient\n",
    "from tqdm import tqdm\n",
    "from litellm import completion\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "DB_NAME = \"preprocessed_db\"\n",
    "collection_name = \"docs\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "#KNOWLEDGE_BASE_PATH = Path(\"knowledge-base\")\n",
    "\n",
    "GOOGLE_DOC_URLS = [\n",
    "    \"https://docs.google.com/document/d/1DKs6IcvDa3xeE-E0DTPUwAIAq3VHBZm7vv4b-rNe1uk/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1TIser-RhtD_sbyOfltpzfQmnYpJV2wpew53R2Ydyd5s/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1QEr8KK0y91MI5PVZ5kUk824yLzV6vzb23TJqHptxPzQ/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/18Sp6koXBKHbo9QA0utYfbDv7TZ-MdkCRV1txNxikt5Y/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1x7Nj6Bt99PMIyJKCDw5b5Vw--WK_rhT4cJE0aXTR4PM/edit?usp=sharing\",\n",
    "    \n",
    "\n",
    "AVERAGE_CHUNK_SIZE = 500\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\n",
    "            \"source\": document[\"source\"],\n",
    "            \"type\": document.get(\"type\", \"google_doc\")   \n",
    "        }\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text, metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a305fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_google_doc_text(doc_url: str) -> str:\n",
    "    doc_id = doc_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    export_url = f\"https://docs.google.com/document/d/{doc_id}/export?format=txt\"\n",
    "    response = requests.get(export_url)\n",
    "    response.raise_for_status()\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5abdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"Fetch documents from Google Docs\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for url in GOOGLE_DOC_URLS:\n",
    "        text = fetch_google_doc_text(url)\n",
    "\n",
    "        documents.append({\n",
    "            \"type\": \"google_doc\",\n",
    "            \"source\": url,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "    return f\"\"\"\n",
    "You take a document and split it into overlapping chunks for a personal programming knowledge base.\n",
    "\n",
    "The document type: {document.get(\"type\", \"google_doc\")}\n",
    "The document source: {document.get(\"source\", \"unknown\")}\n",
    "\n",
    "These chunks will be used by a chatbot to answer the user's programming questions (Python, LLMs, APIs, tooling, debugging, etc.).\n",
    "Split the document in a way that supports retrieval: keep topics coherent, preserve code blocks, and avoid breaking explanations mid-thought.\n",
    "Make sure the entire document is covered by the chunks — do not omit anything.\n",
    "This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
    "Include overlap between chunks (typically ~25% overlap or ~50 words), so key context appears in multiple chunks.\n",
    "\n",
    "For each chunk, provide:\n",
    "- headline: a short, query-friendly title (a few words)\n",
    "- summary: a few sentences summarizing what this chunk helps with (focus on likely questions)\n",
    "- original_text: the exact original text of the chunk (do not rewrite or alter it)\n",
    "\n",
    "Return your answer as JSON in this exact format:\n",
    "{{\"chunks\":[{{\"headline\":\"...\",\"summary\":\"...\",\"original_text\":\"...\"}}, ...]}}\n",
    "\n",
    "Here is the document:\n",
    "\n",
    "{document[\"text\"]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_prompt(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f58850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab04779",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_messages(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    messages = make_messages(document)\n",
    "    response = completion(model=MODEL, messages=messages, response_format=Chunks)\n",
    "    reply = response.choices[0].message.content\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480494d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccab1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(documents):\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents):\n",
    "        chunks.extend(process_document(doc))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93115f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=DB_NAME)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    emb = openai.embeddings.create(model=embedding_model, input=texts).data\n",
    "    vectors = [e.embedding for e in emb]\n",
    "\n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f52038",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=DB_NAME)\n",
    "collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "result = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "\n",
    "vectors = np.array(result[\"embeddings\"])\n",
    "documents = result[\"documents\"]\n",
    "metadatas = result[\"metadatas\"]\n",
    "\n",
    "# Color per document (source)\n",
    "sources = [m.get(\"source\", \"unknown\") for m in metadatas]\n",
    "unique_sources = sorted(set(sources))\n",
    "\n",
    "palette = [\n",
    "    \"blue\", \"green\", \"red\", \"orange\", \"purple\",\n",
    "    \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"\n",
    "]\n",
    "\n",
    "SOURCE_TO_COLOR = {\n",
    "    source: palette[i % len(palette)]\n",
    "    for i, source in enumerate(unique_sources)\n",
    "}\n",
    "\n",
    "colors = [SOURCE_TO_COLOR[source] for source in sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4683c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = vectors.shape[0]\n",
    "perplexity = max(2, min(30, n - 1))  # musi być < n_samples\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Source: {s}<br>Text: {d[:200]}...\" for s, d in zip(sources, documents)],\n",
    "    hoverinfo=\"text\"\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"2D Chroma Vector Store Visualization\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankOrder(BaseModel):\n",
    "    order: list[int] = Field(\n",
    "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_K = 10\n",
    "\n",
    "def fetch_context_unranked(question):\n",
    "    query_vec = openai.embeddings.create(\n",
    "        model=embedding_model,\n",
    "        input=[question]\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # (minimalny) guard: nie proś o więcej niż masz w bazie\n",
    "    k = min(RETRIEVAL_K, collection.count())\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_vec],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    docs = (results.get(\"documents\") or [[]])[0]\n",
    "    metas = (results.get(\"metadatas\") or [[]])[0]\n",
    "    dists = (results.get(\"distances\") or [[None] * len(docs)])[0]\n",
    "\n",
    "    chunks = []\n",
    "    for doc, meta, dist in zip(docs, metas, dists):\n",
    "        if not doc:  # pomiń None / pusty string\n",
    "            continue\n",
    "        meta = dict(meta or {})\n",
    "        meta[\"distance\"] = dist\n",
    "        chunks.append(Result(page_content=doc, metadata=meta))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, chunks):\n",
    "    system_prompt = \"\"\"\n",
    "You are a document re-ranker.\n",
    "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "Reply only as JSON in this exact format: {\"order\":[...]}.\n",
    "Include all the chunk ids you are provided with, reranked.\n",
    "\"\"\"\n",
    "\n",
    "    n = len(chunks)\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"The user has asked the following question:\\n\\n{question}\\n\\n\"\n",
    "        \"Order all the chunks of text by relevance to the question, from most relevant to least relevant. \"\n",
    "        \"Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
    "        f\"Valid chunk ids are integers from 1 to {n}.\\n\\n\"\n",
    "        \"Here are the chunks:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\"\n",
    "\n",
    "    user_prompt += 'Reply only as JSON in this exact format: {\"order\":[...]}'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "    reply = response.choices[0].message.content\n",
    "    order = RankOrder.model_validate_json(reply).order\n",
    "\n",
    "    # --- minimal robustness: keep only valid ids, remove duplicates, add missing ids ---\n",
    "    order = [i for i in order if isinstance(i, int) and 1 <= i <= n]\n",
    "\n",
    "    seen = set()\n",
    "    order = [i for i in order if not (i in seen or seen.add(i))]\n",
    "\n",
    "    missing = [i for i in range(1, n + 1) if i not in order]\n",
    "    order.extend(missing)\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"len(chunks) =\", n)\n",
    "    print(\"order =\", order)\n",
    "    print(\"min/max =\", min(order), max(order))\n",
    "\n",
    "    return [chunks[i - 1] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ed5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Co to są dunder methods\"\n",
    "chunks = fetch_context_unranked(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23594f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in reranked:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405de4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Co robi funkcja void?\"\n",
    "RETRIEVAL_K = 20\n",
    "chunks = fetch_context_unranked(question)\n",
    "for index, c in enumerate(chunks):\n",
    "    if \"void\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22948df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, c in enumerate(reranked):\n",
    "    if \"void\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question):\n",
    "    chunks = fetch_context_unranked(question)\n",
    "    return rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, helpful programming assistant.\n",
    "You are chatting with a user who is asking questions about programming concepts, languages, tools, and software development.\n",
    "\n",
    "Your answers will be evaluated for accuracy, relevance, and completeness.\n",
    "Answer the user's question using ONLY the information provided in the context below.\n",
    "Do not invent information or rely on outside knowledge.\n",
    "If the context does not contain enough information to answer the question, say so clearly.\n",
    "\n",
    "For context, here are specific extracts from the personal programming knowledge base that may be relevant to the user's question:\n",
    "{context}\n",
    "\n",
    "Using only this context, please answer the user's question.\n",
    "Be accurate, relevant, and complete.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_rag_messages(question, history, chunks):\n",
    "    context = \"\\n\\n\".join(f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks)\n",
    "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(question, history=None):\n",
    "    \"\"\"Rewrite the user's question into a short, specific query suitable for searching a personal programming knowledge base.\"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    message = f\"\"\"\n",
    "You are assisting a user by rewriting their question into a concise search query\n",
    "that will be used to look up information in a personal programming knowledge base.\n",
    "\n",
    "This is the history of the conversation so far:\n",
    "{history}\n",
    "\n",
    "This is the user's current question:\n",
    "{question}\n",
    "\n",
    "Respond only with a single, refined, very short query that is most likely\n",
    "to surface relevant programming-related content (concepts, APIs, syntax, behavior).\n",
    "Do not include explanations or extra text.\n",
    "IMPORTANT: Respond ONLY with the search query, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    response = completion(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": message}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d050a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_query(\"Co robi funkcja void?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def answer_question(question: str, history: Optional[list[dict]] = None) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG and return the answer and the retrieved context\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    query = rewrite_query(question, history)\n",
    "    print(query)\n",
    "\n",
    "    chunks = fetch_context(query)\n",
    "    messages = make_rag_messages(question, history, chunks)\n",
    "\n",
    "    response = completion(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Co robi funkcja void?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Co to są dictionary w pythonie?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c93b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"W którym dokumencie znajdę informacje na temat formularzy html\", [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
